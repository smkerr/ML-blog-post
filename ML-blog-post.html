<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007ba5; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #007ba5; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #007ba5; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #5e5e5e; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>

<style>
  div.csl-bib-body { }
  div.csl-entry {
    clear: both;
    }
  .hanging div.csl-entry {
    margin-left:2em;
    text-indent:-2em;
  }
  div.csl-left-margin {
    min-width:2em;
    float:left;
  }
  div.csl-right-inline {
    margin-left:2em;
    padding-left:1em;
  }
  div.csl-indent {
    margin-left: 2em;
  }
</style>

  <!--radix_placeholder_meta_tags-->
  <title>COVID Fake News Detection</title>

  <meta property="description" itemprop="description" content="We construct a fake news detection algorithm using Natural Language Processing. We then use our model to quantify the extent to which these types of machine learning models degrade over time."/>

  <link rel="license" href="https://creativecommons.org/licenses/by/4.0/"/>

  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2022-05-11"/>
  <meta property="article:created" itemprop="dateCreated" content="2022-05-11"/>
  <meta name="article:author" content="Marco Schmidt"/>
  <meta name="article:author" content="Hannah Schweren"/>
  <meta name="article:author" content="Steve Kerr"/>

  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="COVID Fake News Detection"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="We construct a fake news detection algorithm using Natural Language Processing. We then use our model to quantify the extent to which these types of machine learning models degrade over time."/>
  <meta property="og:locale" content="en_US"/>

  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="COVID Fake News Detection"/>
  <meta property="twitter:description" content="We construct a fake news detection algorithm using Natural Language Processing. We then use our model to quantify the extent to which these types of machine learning models degrade over time."/>

  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Fighting an infodemic: Covid-19 fake news dataset;citation_publication_date=2021;citation_publisher=Springer;citation_author=Parth Patwa;citation_author=Shivam Sharma;citation_author=Srinivas Pykl;citation_author=Vineeth Guptha;citation_author=Gitanjali Kumari;citation_author=Md Shad Akhtar;citation_author=Asif Ekbal;citation_author=Amitava Das;citation_author=Tanmoy Chakraborty"/>
  <meta name="citation_reference" content="citation_title=Robust fake news detection over time and attack;citation_publication_date=2019;citation_publisher=Association for Computing Machinery;citation_volume=11;citation_doi=10.1145/3363818;citation_issn=2157-6904;citation_author=Benjamin D. Horne;citation_author=Jeppe Nørregaard;citation_author=Sibel Adali"/>
  <meta name="citation_reference" content="citation_title=A systematic survey on deep learning and machine learning approaches of fake news detection in the pre-and post-COVID-19 pandemic;citation_publication_date=2021;citation_publisher=Emerald Publishing Limited;citation_author=Rajshree Varma;citation_author=Yugandhara Verma;citation_author=Priya Vijayvargiya;citation_author=Prathamesh P Churi"/>
  <!--radix_placeholder_rmarkdown_metadata-->

  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","categories","creative_commons","repository_url","output","preview","bibliography"]}},"value":[{"type":"character","attributes":{},"value":["COVID Fake News Detection"]},{"type":"character","attributes":{},"value":["We construct a fake news detection algorithm using Natural Language Processing. We then use our model to quantify the extent to which these types of machine learning models degrade over time.\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url"]}},"value":[{"type":"character","attributes":{},"value":["Marco Schmidt"]},{"type":"character","attributes":{},"value":["https://github.com/m-schildt"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url"]}},"value":[{"type":"character","attributes":{},"value":["Hannah Schweren"]},{"type":"character","attributes":{},"value":["https://github.com/hannahmagda"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url"]}},"value":[{"type":"character","attributes":{},"value":["Steve Kerr"]},{"type":"character","attributes":{},"value":["https://smkerr.github.io/"]}]}]},{"type":"character","attributes":{},"value":["2022-05-11"]},{"type":"character","attributes":{},"value":["Natural Language Processing"]},{"type":"character","attributes":{},"value":["CC BY"]},{"type":"character","attributes":{},"value":["https://github.com/smkerr/COVID-fake-news-detection"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["figures/fake-news-detection.png"]},{"type":"character","attributes":{},"value":["bibliography.bib"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["bibliography.bib","blog-post-template.pdf","figures/Character_per_tweet.png","figures/count-matrix.png","figures/Diagramm_v2.png","figures/fake-news-detection.png","figures/fake-news-example.png","figures/misclassified-FN.png","figures/misclassified-FP.png","figures/new_data.png","figures/pre-processing.png","figures/sample-tweet-politifact.png","figures/sample-tweet.png","figures/scatterplot_new_data.png","figures/scatterplot_training_data.png","figures/Table_1.png","figures/Table_2.png","figures/Table_3.png","figures/Table_4.png","figures/Table_5.png","figures/Table_6.png","figures/tfidf-matrix.png","figures/Word_frequency.png","figures/word-vectors.png","figures/Words_per_tweet.png","ML-blog-post_files/anchor-4.2.2/anchor.min.js","ML-blog-post_files/bowser-1.9.3/bowser.min.js","ML-blog-post_files/distill-2.2.21/template.v2.js","ML-blog-post_files/header-attrs-2.11/header-attrs.js","ML-blog-post_files/jquery-3.6.0/jquery-3.6.0.js","ML-blog-post_files/jquery-3.6.0/jquery-3.6.0.min.js","ML-blog-post_files/jquery-3.6.0/jquery-3.6.0.min.map","ML-blog-post_files/popper-2.6.0/popper.min.js","ML-blog-post_files/tippy-6.2.7/tippy-bundle.umd.min.js","ML-blog-post_files/tippy-6.2.7/tippy-light-border.css","ML-blog-post_files/tippy-6.2.7/tippy.css","ML-blog-post_files/tippy-6.2.7/tippy.umd.min.js","ML-blog-post_files/webcomponents-2.0.0/webcomponents.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

  <style type="text/css">

  body {
    background-color: white;
  }

  .pandoc-table {
    width: 100%;
  }

  .pandoc-table>caption {
    margin-bottom: 10px;
  }

  .pandoc-table th:not([align]) {
    text-align: left;
  }

  .pagedtable-footer {
    font-size: 15px;
  }

  d-byline .byline {
    grid-template-columns: 2fr 2fr;
  }

  d-byline .byline h3 {
    margin-block-start: 1.5em;
  }

  d-byline .byline .authors-affiliations h3 {
    margin-block-start: 0.5em;
  }

  .authors-affiliations .orcid-id {
    width: 16px;
    height:16px;
    margin-left: 4px;
    margin-right: 4px;
    vertical-align: middle;
    padding-bottom: 2px;
  }

  d-title .dt-tags {
    margin-top: 1em;
    grid-column: text;
  }

  .dt-tags .dt-tag {
    text-decoration: none;
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0em 0.4em;
    margin-right: 0.5em;
    margin-bottom: 0.4em;
    font-size: 70%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  d-article table.gt_table td,
  d-article table.gt_table th {
    border-bottom: none;
  }

  .html-widget {
    margin-bottom: 2.0em;
  }

  .l-screen-inset {
    padding-right: 16px;
  }

  .l-screen .caption {
    margin-left: 10px;
  }

  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .shaded-content {
    background: white;
  }

  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }

  .hidden {
    display: none !important;
  }

  d-article {
    padding-top: 2.5rem;
    padding-bottom: 30px;
  }

  d-appendix {
    padding-top: 30px;
  }

  d-article>p>img {
    width: 100%;
  }

  d-article h2 {
    margin: 1rem 0 1.5rem 0;
  }

  d-article h3 {
    margin-top: 1.5rem;
  }

  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }

  /* Tweak code blocks */

  d-article div.sourceCode code,
  d-article pre code {
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  }

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: auto;
  }

  d-article div.sourceCode {
    background-color: white;
  }

  d-article div.sourceCode pre {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }

  d-article pre {
    font-size: 12px;
    color: black;
    background: none;
    margin-top: 0;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  d-article pre a {
    border-bottom: none;
  }

  d-article pre a:hover {
    border-bottom: none;
    text-decoration: underline;
  }

  d-article details {
    grid-column: text;
    margin-bottom: 0.8em;
  }

  @media(min-width: 768px) {

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: visible !important;
  }

  d-article div.sourceCode pre {
    padding-left: 18px;
    font-size: 14px;
  }

  d-article pre {
    font-size: 14px;
  }

  }

  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  /* CSS for d-contents */

  .d-contents {
    grid-column: text;
    color: rgba(0,0,0,0.8);
    font-size: 0.9em;
    padding-bottom: 1em;
    margin-bottom: 1em;
    padding-bottom: 0.5em;
    margin-bottom: 1em;
    padding-left: 0.25em;
    justify-self: start;
  }

  @media(min-width: 1000px) {
    .d-contents.d-contents-float {
      height: 0;
      grid-column-start: 1;
      grid-column-end: 4;
      justify-self: center;
      padding-right: 3em;
      padding-left: 2em;
    }
  }

  .d-contents nav h3 {
    font-size: 18px;
    margin-top: 0;
    margin-bottom: 1em;
  }

  .d-contents li {
    list-style-type: none
  }

  .d-contents nav > ul {
    padding-left: 0;
  }

  .d-contents ul {
    padding-left: 1em
  }

  .d-contents nav ul li {
    margin-top: 0.6em;
    margin-bottom: 0.2em;
  }

  .d-contents nav a {
    font-size: 13px;
    border-bottom: none;
    text-decoration: none
    color: rgba(0, 0, 0, 0.8);
  }

  .d-contents nav a:hover {
    text-decoration: underline solid rgba(0, 0, 0, 0.6)
  }

  .d-contents nav > ul > li > a {
    font-weight: 600;
  }

  .d-contents nav > ul > li > ul {
    font-weight: inherit;
  }

  .d-contents nav > ul > li > ul > li {
    margin-top: 0.2em;
  }


  .d-contents nav ul {
    margin-top: 0;
    margin-bottom: 0.25em;
  }

  .d-article-with-toc h2:nth-child(2) {
    margin-top: 0;
  }


  /* Figure */

  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }

  .figure img {
    width: 100%;
  }

  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }

  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }

  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }

  /* Citations */

  d-article .citation {
    color: inherit;
    cursor: inherit;
  }

  div.hanging-indent{
    margin-left: 1em; text-indent: -1em;
  }

  /* Citation hover box */

  .tippy-box[data-theme~=light-border] {
    background-color: rgba(250, 250, 250, 0.95);
  }

  .tippy-content > p {
    margin-bottom: 0;
    padding: 2px;
  }


  /* Tweak 1000px media break to show more text */

  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }

    .grid {
      grid-column-gap: 16px;
    }

    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }

  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }

    .grid {
      grid-column-gap: 32px;
    }
  }


  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */

  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  /* Include appendix styles here so they can be overridden */

  d-appendix {
    contain: layout style;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-top: 60px;
    margin-bottom: 0;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    color: rgba(0,0,0,0.5);
    padding-top: 60px;
    padding-bottom: 48px;
  }

  d-appendix h3 {
    grid-column: page-start / text-start;
    font-size: 15px;
    font-weight: 500;
    margin-top: 1em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.65);
  }

  d-appendix h3 + * {
    margin-top: 1em;
  }

  d-appendix ol {
    padding: 0 0 0 15px;
  }

  @media (min-width: 768px) {
    d-appendix ol {
      padding: 0 0 0 30px;
      margin-left: -30px;
    }
  }

  d-appendix li {
    margin-bottom: 1em;
  }

  d-appendix a {
    color: rgba(0, 0, 0, 0.6);
  }

  d-appendix > * {
    grid-column: text;
  }

  d-appendix > d-footnote-list,
  d-appendix > d-citation-list,
  d-appendix > distill-appendix {
    grid-column: screen;
  }

  /* Include footnote styles here so they can be overridden */

  d-footnote-list {
    contain: layout style;
  }

  d-footnote-list > * {
    grid-column: text;
  }

  d-footnote-list a.footnote-backlink {
    color: rgba(0,0,0,0.3);
    padding-left: 0.5em;
  }



  /* Anchor.js */

  .anchorjs-link {
    /*transition: all .25s linear; */
    text-decoration: none;
    border-bottom: none;
  }
  *:hover > .anchorjs-link {
    margin-left: -1.125em !important;
    text-decoration: none;
    border-bottom: none;
  }

  /* Social footer */

  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }

  .disqus-comments {
    margin-right: 30px;
  }

  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }

  #disqus_thread {
    margin-top: 30px;
  }

  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }

  .article-sharing a:hover {
    border-bottom: none;
  }

  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }

  .subscribe p {
    margin-bottom: 0.5em;
  }


  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }


  .sidebar-section.custom {
    font-size: 12px;
    line-height: 1.6em;
  }

  .custom p {
    margin-bottom: 0.5em;
  }

  /* Styles for listing layout (hide title) */
  .layout-listing d-title, .layout-listing .d-title {
    display: none;
  }

  /* Styles for posts lists (not auto-injected) */


  .posts-with-sidebar {
    padding-left: 45px;
    padding-right: 45px;
  }

  .posts-list .description h2,
  .posts-list .description p {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  }

  .posts-list .description h2 {
    font-weight: 700;
    border-bottom: none;
    padding-bottom: 0;
  }

  .posts-list h2.post-tag {
    border-bottom: 1px solid rgba(0, 0, 0, 0.2);
    padding-bottom: 12px;
  }
  .posts-list {
    margin-top: 60px;
    margin-bottom: 24px;
  }

  .posts-list .post-preview {
    text-decoration: none;
    overflow: hidden;
    display: block;
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    padding: 24px 0;
  }

  .post-preview-last {
    border-bottom: none !important;
  }

  .posts-list .posts-list-caption {
    grid-column: screen;
    font-weight: 400;
  }

  .posts-list .post-preview h2 {
    margin: 0 0 6px 0;
    line-height: 1.2em;
    font-style: normal;
    font-size: 24px;
  }

  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.4em;
    font-size: 16px;
  }

  .posts-list .post-preview .thumbnail {
    box-sizing: border-box;
    margin-bottom: 24px;
    position: relative;
    max-width: 500px;
  }
  .posts-list .post-preview img {
    width: 100%;
    display: block;
  }

  .posts-list .metadata {
    font-size: 12px;
    line-height: 1.4em;
    margin-bottom: 18px;
  }

  .posts-list .metadata > * {
    display: inline-block;
  }

  .posts-list .metadata .publishedDate {
    margin-right: 2em;
  }

  .posts-list .metadata .dt-authors {
    display: block;
    margin-top: 0.3em;
    margin-right: 2em;
  }

  .posts-list .dt-tags {
    display: block;
    line-height: 1em;
  }

  .posts-list .dt-tags .dt-tag {
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0.3em 0.4em;
    margin-right: 0.2em;
    margin-bottom: 0.4em;
    font-size: 60%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  .posts-list img {
    opacity: 1;
  }

  .posts-list img[data-src] {
    opacity: 0;
  }

  .posts-more {
    clear: both;
  }


  .posts-sidebar {
    font-size: 16px;
  }

  .posts-sidebar h3 {
    font-size: 16px;
    margin-top: 0;
    margin-bottom: 0.5em;
    font-weight: 400;
    text-transform: uppercase;
  }

  .sidebar-section {
    margin-bottom: 30px;
  }

  .categories ul {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }

  .categories li {
    color: rgba(0, 0, 0, 0.8);
    margin-bottom: 0;
  }

  .categories li>a {
    border-bottom: none;
  }

  .categories li>a:hover {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  }

  .categories .active {
    font-weight: 600;
  }

  .categories .category-count {
    color: rgba(0, 0, 0, 0.4);
  }


  @media(min-width: 768px) {
    .posts-list .post-preview h2 {
      font-size: 26px;
    }
    .posts-list .post-preview .thumbnail {
      float: right;
      width: 30%;
      margin-bottom: 0;
    }
    .posts-list .post-preview .description {
      float: left;
      width: 45%;
    }
    .posts-list .post-preview .metadata {
      float: left;
      width: 20%;
      margin-top: 8px;
    }
    .posts-list .post-preview p {
      margin: 0 0 12px 0;
      line-height: 1.5em;
      font-size: 16px;
    }
    .posts-with-sidebar .posts-list {
      float: left;
      width: 75%;
    }
    .posts-with-sidebar .posts-sidebar {
      float: right;
      width: 20%;
      margin-top: 60px;
      padding-top: 24px;
      padding-bottom: 24px;
    }
  }


  /* Improve display for browsers without grid (IE/Edge <= 15) */

  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }

  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }

  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }

  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }

  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }

  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }

  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }


  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }

  .downlevel .footnotes ol {
    padding-left: 13px;
  }

  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }

  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }

  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }

  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  .downlevel .posts-list .post-preview {
    color: inherit;
  }



  </style>

  <script type="application/javascript">

  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }

  // show body when load is complete
  function on_load_complete() {

    // add anchors
    if (window.anchors) {
      window.anchors.options.placement = 'left';
      window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
    }


    // set body to visible
    document.body.style.visibility = 'visible';

    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }

    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }

  function init_distill() {

    init_common();

    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);

    // create d-title
    $('.d-title').changeElementType('d-title');

    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);

    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();

    // move posts container into article
    $('.posts-container').appendTo($('d-article'));

    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');

    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;

    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();

    // move refs into #references-listing
    $('#references-listing').replaceWith($('#refs'));

    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-contents a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });

    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');

    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {

      // capture layout
      var layout = $(this).attr('data-layout');

      // apply layout to markdown level block elements
      var elements = $(this).children().not('details, div.sourceCode, pre, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });


      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });

    // remove code block used to force  highlighting css
    $('.distill-force-highlighting-css').parent().remove();

    // remove empty line numbers inserted by pandoc when using a
    // custom syntax highlighting theme
    $('code.sourceCode a:empty').remove();

    // load distill framework
    load_distill_framework();

    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {

      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;

      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');

      // article with toc class
      $('.d-contents').parent().addClass('d-article-with-toc');

      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

      // add orcid ids
      $('.authors-affiliations').find('.author').each(function(i, el) {
        var orcid_id = front_matter.authors[i].orcidID;
        if (orcid_id) {
          var a = $('<a></a>');
          a.attr('href', 'https://orcid.org/' + orcid_id);
          var img = $('<img></img>');
          img.addClass('orcid-id');
          img.attr('alt', 'ORCID ID');
          img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
          a.append(img);
          $(this).append(a);
        }
      });

      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }

      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");

      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }

       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }

      // remove d-appendix and d-footnote-list local styles
      $('d-appendix > style:first-child').remove();
      $('d-footnote-list > style:first-child').remove();

      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();

      // hoverable references
      $('span.citation[data-cites]').each(function() {
        var refs = $(this).attr('data-cites').split(" ");
        var refHtml = refs.map(function(ref) {
          return "<p>" + $('#ref-' + ref).html() + "</p>";
        }).join("\n");
        window.tippy(this, {
          allowHTML: true,
          content: refHtml,
          maxWidth: 500,
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        });
      });

      // clear polling timer
      clearInterval(tid);

      // show body now that everything is ready
      on_load_complete();
    }

    var tid = setInterval(distill_post_process, 50);
    distill_post_process();

  }

  function init_downlevel() {

    init_common();

     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));

    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;

    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();

    // remove toc
    $('.d-contents').remove();

    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });


    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);

    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();

    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });

    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));

    $('body').addClass('downlevel');

    on_load_complete();
  }


  function init_common() {

    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};

        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });

        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);

    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});

    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });

      }
    });

    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });

    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }

    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');

    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");

    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();

    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }

  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });

  </script>

  <!--/radix_placeholder_distill-->
  <script src="ML-blog-post_files/header-attrs-2.11/header-attrs.js"></script>
  <script src="ML-blog-post_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="ML-blog-post_files/popper-2.6.0/popper.min.js"></script>
  <link href="ML-blog-post_files/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="ML-blog-post_files/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="ML-blog-post_files/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="ML-blog-post_files/anchor-4.2.2/anchor.min.js"></script>
  <script src="ML-blog-post_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="ML-blog-post_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="ML-blog-post_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"COVID Fake News Detection","description":"We construct a fake news detection algorithm using Natural Language Processing. We then use our model to quantify the extent to which these types of machine learning models degrade over time.","authors":[{"author":"Marco Schmidt","authorURL":"https://github.com/m-schildt","affiliation":"&nbsp;","affiliationURL":"#","orcidID":""},{"author":"Hannah Schweren","authorURL":"https://github.com/hannahmagda","affiliation":"&nbsp;","affiliationURL":"#","orcidID":""},{"author":"Steve Kerr","authorURL":"https://smkerr.github.io/","affiliation":"&nbsp;","affiliationURL":"#","orcidID":""}],"publishedDate":"2022-05-11T00:00:00.000+02:00","citationText":"Schmidt, et al., 2022"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>COVID Fake News Detection</h1>
<!--radix_placeholder_categories-->
<div class="dt-tags">
<div class="dt=tag">Natural Language Processing</div>
</div>
<!--/radix_placeholder_categories-->
<p><p>We construct a fake news detection algorithm using Natural Language Processing. We then use our model to quantify the extent to which these types of machine learning models degrade over time.</p></p>
</div>

<div class="d-byline">
  Marco Schmidt <a href="https://github.com/m-schildt" class="uri">https://github.com/m-schildt</a> 
  
,   Hannah Schweren <a href="https://github.com/hannahmagda" class="uri">https://github.com/hannahmagda</a> 
  
,   Steve Kerr <a href="https://smkerr.github.io/" class="uri">https://smkerr.github.io/</a> 
  
<br/>2022-05-11
</div>

<div class="d-article">
<div class="layout-chunk" data-layout="l-body">
<p><img src="figures/fake-news-detection.png" width="100%" /></p>
</div>
<h2 id="abstract">Abstract</h2>
<p>Fake news represents one of the most pressing issues faced by democracies in the digital age. While there have been many examples of fake news consumption leading to the adoption of inaccurate beliefs and harmful behaviors, fake news regarding the Covid-19 pandemic is especially dangerous due to its potential to impact public health outcomes.</p>
<aside>
The code for our project can be accessed <a href="https://github.com/smkerr/COVID-fake-news-detection">here</a>.
</aside>
<p>While machine learning has advanced our ability to identify and root out fake news, this approach is not without its limitations. Due to the constantly evolving nature of fake news, a fake news detection algorithm which is highly effective at one point in time may perform significantly worse later in time. This blog post describes the approach we took in our project and complements our final report which centers on the following question: <strong>To what extent does the fast-moving nature of fake news represent a limitation for fake news detection algorithms?</strong></p>
<p>To answer this question, we construct our own fake news detection algorithm using the existing <a href="https://paperswithcode.com/dataset/covid-19-fake-news-dataset">Covid-19 Fake News dataset</a> which comprises 10,700 Covid-related social media posts which have been labeled either “real” or “fake.” Our model achieved a score of 93.5% when applied to social media posts from the same time period as the one on which it was trained, and received a score of 81.4% when applied to more recent social media posts. This roughly 10% decrease in performance supports our assumption that fake news detection algorithms degrade over time, albeit less quickly than we initially anticipated.</p>
<h2 id="background">Background</h2>
<p>Misinformation has always been a threat to society and democracy. Today, this danger is fueled by the potential of its rapid diffusion through social media. Machine learning (ML) and Natural Language Processing (NLP) enable us to detect fake news so that we might warn the reader and raise awareness about misinformation. This blog post will therefore present our work on the identification of such Fake News on social media platforms.</p>
<p>While some ML models are already very efficient at classifying whether news is real or fake, such models still face limitations. ML models are often bound to the data upon which they were originally trained, which is why the fast-moving nature of online content could degrade their effectiveness. Seeing as fake news is highly influenced by current events, we want to test the assumption that temporally rigid training sets are prone to a decline in effectiveness. As a current, politicized, and highly relevant topic, Covid-19 has created room for misinformation and intentional manipulation in social media, thereby representing an interesting case study.</p>
<p>This blog post will walk you through the two main components of our work:</p>
<ol type="1">
<li>Building a fake news detection algorithm with a high level of effectiveness using the <a href="https://paperswithcode.com/dataset/covid-19-fake-news-dataset">Covid-19 Fake News dataset</a>.</li>
<li>Applying our fake news detection algorithm to more recent data which we have collected ourselves to measure the extent to which the model degrades over time.</li>
</ol>
<h2 id="related-work">Related Work</h2>
<p>As mentioned in the introduction, the idea for the first part of our project is based on the paper of <em>Patwa et al.</em> <span class="citation" data-cites="patwa2021fighting">(<a href="#ref-patwa2021fighting" role="doc-biblioref">Patwa et al. 2021</a>)</span> which provided a good starting point for our further experimenting with the original and the new dataset. They achieve their best result with a Support Vector Machine classifier, scoring 92.8%.</p>
<p>The literature offers many similar approaches, for example, <em>Varma, Rajshree et al.</em> conducted a review of various ML and Deep Learning methods applied to the same topic coming to the conclusion that “Naive Bayes, Support Vector Machine, and Logistic regression are the most widely used supervised ML algorithms for developing fake news detection models” <span class="citation" data-cites="varma2021systematic">(<a href="#ref-varma2021systematic" role="doc-biblioref">Varma et al. 2021</a>)</span>.</p>
<p>We hypothesized that there would be a reduction in the predictive power of our model over time. This assumption was based on previous research on the phenomenon of “concept drift” <span class="citation" data-cites="horne2020">(<a href="#ref-horne2020" role="doc-biblioref">Horne, Nørregaard, and Adali 2019</a>)</span>. We assume that fake news constitutes non-stationary data whose characteristics might shift over time and thus a model trained on current data cannot provide long term reliable prediction which would pose a threat to the application of such models.</p>
<h2 id="proposed-method">Proposed Method</h2>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:workflow"></span>
<img src="figures/Diagramm_v2.png" alt="Workflow" width="100%" />
<p class="caption">
Figure 1: Workflow
</p>
</div>
</div>
<p>In order to measure concept drift, we require datasets from two distinct time periods. We use the <a href="https://paperswithcode.com/dataset/covid-19-fake-news-dataset">Covid-19 Fake News Detection</a> dataset to train our model and to baseline its performance. We then apply our algorithm to a dataset we’ve created ourselves which contains Covid-related social media posts from early 2022 in order to determine the extent to which our model’s performance declines over time. Here’s a sample tweet to demonstrate our approach.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-2"></span>
<img src="figures/sample-tweet.png" alt="Sample Tweet" width="100%" />
<p class="caption">
Figure 2: Sample Tweet
</p>
</div>
</div>
<p>Our sample tweet comes from U.S. Representative Chip Roy who characterizes Pfizer’s request for vaccine authorization for children younger than five as having “zero basis in science.” Experts from PolitiFact, a widely recognized fact-checking site, assign this claim a “Pants on Fire!” rating which is reserved for inaccurate statements which make ridiculous claims.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-3"></span>
<img src="figures/sample-tweet-politifact.png" alt="PolitiFact Rating" width="100%" />
<p class="caption">
Figure 3: PolitiFact Rating
</p>
</div>
</div>
<p>We will use this tweet as an example to demonstrate our pre-processing pipeline. This process involves lower-casing, tokenizing, stop word removal, and lemmatization. The motivation here is to reduce the contents of each tweet in such a way that we maximize the amount of useful information while minimizing the amount of noise.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sample tweet</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>tweet <span class="op">=</span> <span class="st">&quot;Literally using the force of government and the culture of fear to jab children under 5 - with zero basis in science - to make billions of dollars. Shameful, @pfizer.&quot;</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># lowercase </span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>tweet_lower <span class="op">=</span> tweet.lower() </span></code></pre></div>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>literally using the force of government and the culture of fear to jab children under 5 - with zero basis in science - to make billions of dollars. shameful, @pfizer.</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>tweet_tidy1 <span class="op">=</span> re.sub(<span class="vs">r&quot;http(\S)+&quot;</span>,<span class="st">&#39; &#39;</span>, tweet_lower) <span class="co"># remove URLs   </span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>tweet_tidy2 <span class="op">=</span> re.sub(<span class="vs">r&quot;www(\S)+&quot;</span>,<span class="st">&#39; &#39;</span>, tweet_tidy1) <span class="co"># remove URLs</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>tweet_tidy3 <span class="op">=</span> re.sub(<span class="vs">r&quot;&amp;&quot;</span>,<span class="st">&#39; and &#39;</span>, tweet_tidy2) <span class="co"># replace &amp; with &#39; and &#39;</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>tweet_tidy4 <span class="op">=</span> tweet_tidy3.replace(<span class="st">&#39;&amp;amp&#39;</span>,<span class="st">&#39; &#39;</span>) <span class="co"># replace &amp;amp with &#39; &#39;</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>tweet_tidy5 <span class="op">=</span> re.sub(<span class="vs">r&quot;[^0-9a-zA-Z]+&quot;</span>,<span class="st">&#39; &#39;</span>, tweet_tidy4) <span class="co"># remove non-alphanumeric characters</span></span></code></pre></div>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>literally using the force of government and the culture of fear to jab children under 5 with zero basis in science to make billions of dollars shameful pfizer </code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>tweet_tokenized <span class="op">=</span> tweet_tidy5.split() <span class="co"># tokenize </span></span></code></pre></div>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>[&#39;literally&#39;, &#39;using&#39;, &#39;the&#39;, &#39;force&#39;, &#39;of&#39;, &#39;government&#39;, &#39;and&#39;, &#39;the&#39;, &#39;culture&#39;, &#39;of&#39;, &#39;fear&#39;, &#39;to&#39;, &#39;jab&#39;, &#39;children&#39;, &#39;under&#39;, &#39;5&#39;, &#39;with&#39;, &#39;zero&#39;, &#39;basis&#39;, &#39;in&#39;, &#39;science&#39;, &#39;to&#39;, &#39;make&#39;, &#39;billions&#39;, &#39;of&#39;, &#39;dollars&#39;, &#39;shameful&#39;, &#39;pfizer&#39;]</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>tweet_stopped <span class="op">=</span> [w <span class="cf">for</span> w <span class="kw">in</span> tweet_tokenized <span class="cf">if</span> <span class="kw">not</span> w <span class="kw">in</span> stoplist] <span class="co"># remove stop words</span></span></code></pre></div>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>[&#39;literally&#39;, &#39;using&#39;, &#39;force&#39;, &#39;government&#39;, &#39;culture&#39;, &#39;fear&#39;, &#39;jab&#39;, &#39;children&#39;, &#39;5&#39;, &#39;zero&#39;, &#39;basis&#39;, &#39;science&#39;, &#39;make&#39;, &#39;billions&#39;, &#39;dollars&#39;, &#39;shameful&#39;, &#39;pfizer&#39;]</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>tweet_processed <span class="op">=</span> [lemmatizer.lemmatize(w) <span class="cf">for</span> w <span class="kw">in</span> tweet_stopped] <span class="co"># lemmatize </span></span></code></pre></div>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>[&#39;literally&#39;, &#39;using&#39;, &#39;force&#39;, &#39;government&#39;, &#39;culture&#39;, &#39;fear&#39;, &#39;jab&#39;, &#39;child&#39;, &#39;5&#39;, &#39;zero&#39;, &#39;basis&#39;, &#39;science&#39;, &#39;make&#39;, &#39;billion&#39;, &#39;dollar&#39;, &#39;shameful&#39;, &#39;pfizer&#39;]</code></pre>
</div>
<p>Once the text data has been pre-processed, we begin to extract the features upon which we will train our model. Since computers can’t understand words, we translate our text data into numerical data. We use <em>sklearn</em>’s CountVectorizer to create a matrix containing information on the number of times a word appears. In this count matrix, each row represents a social media post and each column represents a word contained in the dataset. Here’s what that looks like for an example dataset containing four fake social media posts.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create corpus of tweets</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> [<span class="st">&quot;Literally using the force of government and the culture of fear to jab children under 5 - with zero basis in science - to make billions of dollars. Shameful, @pfizer.&quot;</span>,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>          <span class="st">&quot;Dr. Anthony Fauci said lockdowns are a method for coercing people to comply with COVID-19 vaccinations.&quot;</span>,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>          <span class="st">&quot;The drug labels for the Pfizer COVID-19 vaccine &#39;were blank when they should have contained all these diseases and adverse events&#39; listed in a confidential report.&quot;</span>,</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>          <span class="st">&quot;Pfizer paid &#39;$2.8 million bribe payment&#39; to the FDA for COVID-19 vaccine approval.&quot;</span>]</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># count vectorizer</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> CountVectorizer() <span class="co"># count term frequency</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co"># fit &amp; transform tweet into count vector</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>count_matrix <span class="op">=</span> cv.fit_transform(corpus_clean)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co"># convert count matrix to data frame</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>count_array <span class="op">=</span> count_matrix.toarray()</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>tweet_cv_df <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>count_array,columns <span class="op">=</span> cv.get_feature_names())</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="co"># display count matrix</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>tweet_cv_df.head() </span></code></pre></div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-16"></span>
<img src="figures/count-matrix.png" alt="Count Matrix" width="100%" />
<p class="caption">
Figure 4: Count Matrix
</p>
</div>
</div>
<p>While simply creating a data frame of term frequencies may be sufficient, we can improve upon this with a technique known as term frequency-inverse document frequency or TF-IDF for short. While the acronym is a bit of a mouthful, the concept behind it is quite simple. In plain terms, TF-IDF takes a count matrix like the one we just created as input and weights each term by how frequently that term appears in the overall collection of tweets.</p>
<p>Words which are used infrequently receive a higher weighting than words that are commonplace. For example, the word “Covid” appears frequently in our dataset, so it would receive a higher weighting than a less common word such as “Ivermectin.” As such, we are telling the computer to pay special attention to words which are less common. Here’s what that looks like continuing along with the example from above.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># TF-IDF transformer</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>tfidf <span class="op">=</span> TfidfTransformer() <span class="co"># weight counts by word frequency</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># fit &amp; transform count matrix to TF-IDF matrix</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>tweet_tfidf <span class="op">=</span> tfidf.fit_transform(tweet_cv)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># convert to data frame</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>tfidf_array <span class="op">=</span> tweet_tfidf.toarray()</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>tweet_tfidf <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>tfidf_array,columns <span class="op">=</span> cv.get_feature_names())</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co"># display TF-IDF matrix</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>tweet_tfidf.head()</span></code></pre></div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-18"></span>
<img src="figures/tfidf-matrix.png" alt="TF-IDF Matrix" width="100%" />
<p class="caption">
Figure 5: TF-IDF Matrix
</p>
</div>
</div>
<p>Aside from mere word counts, we can extract additional features from our text data by converting each word to a numerical representation of that word’s meaning, also referred to as a “word vector.” To accomplish this, we use <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a>, an unsupervised learning algorithm capable of encoding the co-occurrence probability ratio between two words as vector differences. In practical terms, this enables the computer to derive meanings from the words in our dataset. Since we deemed training our model on each and every word in our dataset to be too computationally expensive, we instead compute one word vector for each social media post by averaging the word vectors from each word contained in that post. Continuing with the example from above, here’s the code implementation.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy </span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">&quot;en_core_web_md&quot;</span>) <span class="co"># load reduced word vector table with 20k unique vectors for ~500k words</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># create list of word vectors</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>word2vec_list <span class="op">=</span> [nlp(doc).vector.reshape(<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>) <span class="cf">for</span> doc <span class="kw">in</span> corpus]</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># join word vectors</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>word2vec_data <span class="op">=</span> np.concatenate(word2vec_list) </span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co"># convert to data frame</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>word2vec_df <span class="op">=</span> pd.DataFrame(word2vec_data) </span></code></pre></div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-20"></span>
<img src="figures/word-vectors.png" alt="Word Vectors" width="100%" />
<p class="caption">
Figure 6: Word Vectors
</p>
</div>
</div>
<p>Now that we’ve extracted features from the text data, it’s time to train our fake news detection algorithm. To do so, we tested several common classification algorithms and used the best one as our baseline. This served as a starting point for the further finetuning of the classifiers on the validation data.</p>
<p>Once we have tuned and tested our models, we apply our best model to our more recent dataset to compare the performance. In doing so, we test our hypothesis by evaluating the extent to which concept drift (and possibly other factors) have degraded the quality of our fake news detection algorithm.</p>
<h2 id="experiments">Experiments</h2>
<p><strong>Original Data</strong>: We use the Covid-19 Fake News Detection dataset created in 2020 by <em>Patwa et al.</em> and featured in the Constraint@AAAI2021 competition <span class="citation" data-cites="patwa2021fighting">(<a href="#ref-patwa2021fighting" role="doc-biblioref">Patwa et al. 2021</a>)</span>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> The dataset comprises 10,700 Covid-related social media posts from various platforms including Twitter, Facebook, and Instagram. Each post contains a falsifiable statement which has been labeled either “real” or “fake.”</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>   id                                              tweet label
0   1  The CDC currently reports 99031 deaths. In gen...  real
1   2  States reported 1121 deaths a small rise from ...  real
2   3  Politically Correct Woman (Almost) Uses Pandem...  fake
3   4  #IndiaFightsCorona: We have 1524 #COVID testin...  real
4   5  Populous states can generate large case counts...  real</code></pre>
</div>
<p><em>Patwa et al.</em>’s Covid-19 Fake News Detection dataset has already been split into training (60%), test (20%), and validation sets (20%). The dataset is approximately balanced between real (52.3%) and fake (47.7%). Details regarding the distribution of real and fake posts across training, test, and validation sets can be found in Figure 7.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-22"></span>
<img src="figures/Table_1.png" alt="Dataset composition before pre-processing" width="100%" />
<p class="caption">
Figure 7: Dataset composition before pre-processing
</p>
</div>
</div>
<p>“Fake” claims are collected from various fact-checking websites such as PolitiFact, Snopes, Boomlive, etc. and from tools like Google fact-check-explorer and IFCN chatbot. “Real” claims include tweets from credible and relevant Twitter accounts such as the World Health Organization (WHO) and the Centers for Disease Control and Prevention (CDC) among others.</p>
<p>We acknowledge the shortcomings associated with sourcing “real” data exclusively from Twitter while sourcing “fake” data from various social media platforms. To correct for this, we remove all social media posts exceeding Twitter’s 280 character limit from the dataset during pre-processing in an effort to make the “real” and “fake” data more comparable. While this did reduce the total number of observations, the amount of data is still sufficient to proceed. Moreover, we maintain the roughly equal split between real (49.1%) and fake (50.9%) social media posts in the dataset. See Figure 8 for a more detailed breakdown.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-23"></span>
<img src="figures/Table_2.png" alt="Dataset composition after pre-processing" width="100%" />
<p class="caption">
Figure 8: Dataset composition after pre-processing
</p>
</div>
</div>
<p>To familiarize ourselves with the data and to detect potential biases, we conducted an exploratory analysis. Our analysis revealed that the number of characters (Figure 9) and the number of words (Figure 10) are approximately balanced between real and fake posts.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-24"></span>
<img src="figures/Character_per_tweet.png" alt="Characters per tweet" width="100%" />
<p class="caption">
Figure 9: Characters per tweet
</p>
</div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-25"></span>
<img src="figures/Words_per_tweet.png" alt="Words per tweet" width="100%" />
<p class="caption">
Figure 10: Words per tweet
</p>
</div>
</div>
<p>Additionally, we examined high-frequency words (Figure 11) for both fake and real claims to gain further insight into the data. No significant discrepancies could be identified.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-26"></span>
<img src="figures/Word_frequency.png" alt="Word frequency" width="100%" />
<p class="caption">
Figure 11: Word frequency
</p>
</div>
</div>
<p>Lastly, we used part-of-speech (POS) tagging to gain a deeper understanding of the tweets. Besides a slight imbalance in the frequency of nouns between the real (51.3%) and fake (59.7%) posts, the overall balance gave no indication of significant biases in the dataset (Figure 12).</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-27"></span>
<img src="figures/Table_3.png" alt="POS Tag Counts" width="100%" />
<p class="caption">
Figure 12: POS Tag Counts
</p>
</div>
</div>
<p>In sum, our exploratory analysis did not uncover any significant underlying issues with the Covid-19 Fake News Detection dataset, thereby allowing us to proceed with our project.</p>
<p><strong>New data</strong>: For the second part of the project, we created our own dataset of more recent fake and real posts which were published between January and April 2022. We employed methods which closely mirror those used in the original paper by <em>Patwa et al.</em> so as to ensure that our dataset is comparable to the original dataset in all aspects aside from time period <span class="citation" data-cites="patwa2021fighting">(<a href="#ref-patwa2021fighting" role="doc-biblioref">Patwa et al. 2021</a>)</span>. Thus, we collected tweets from the same Twitter accounts and used Politifact.com as a source for fake news, as the authors did for the original dataset.</p>
<p>While the authors of the original dataset did use additional fact-checking sources beyond PolitiFact when compiling fake claims such as Snopes, Boomlive, and Google fact-check-explorer among others, due to time constraints, our project only used data from PolitiFact.com. Despite this, we were still able to create a large enough dataset to draw conclusions. In the creation of our dataset, we took special care to maintain an equal share of fake and real posts within the dataset (the final new dataset contains 100 real and 100 fake posts). An overview of the data collection process is provided in Figure 13.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:fig8"></span>
<img src="figures/new_data.png" alt="Data collection workflow" width="100%" />
<p class="caption">
Figure 13: Data collection workflow
</p>
</div>
</div>
<p><strong>Evaluation method</strong>: The fake news detection algorithm that we build during the first part of our project is evaluated using a F1-score. We consider our fine-tuned model to be successful if it achieves a higher F1-score than our baseline model which received a F1-score of 92.8%.</p>
<p>The F1-score is also present in the second part of our project where we attempt to quantify the amount of concept drift associated with Covid-related fake news. We apply our fine-tuned fake news detection algorithm which was developed during the first part of the project to both the original dataset and the new dataset which we have created ourselves. The difference between the two F1-scores indicates the magnitude of the concept drift associated with Covid-related fake news between the two time periods in which the datasets were created.</p>
<p>While we acknowledge that there are several reasons which could explain the difference in F1-scores such as inconsistencies in the way both datasets were created, we believe that any gap between the two F1-scores contains valuable information regarding the magnitude to which concept drift may be present.</p>
<p><strong>Software</strong>: The project was implemented with Python. Besides basic dependencies such as <em>pandas</em> and <em>numpy</em>, we use <em>scikit-learn</em>, <em>NLTK</em>, and <em>SpaCy</em> packages to pre-process text data, extract features, and train our models.</p>
<p><strong>Experimental details</strong>: When creating our baseline model, we selected off-the-shelf models including Logistic Regression, Decision Tree, Gradient Boosting, and Linear SVC classifiers. We used default parameters for all models when identifying the most highest-scoring model (based on on F1-score) to serve as our baseline.</p>
<p>For the word embedding feature extraction process, we relied upon <em>SpaCy</em>’s English-language reduced word vector table with 20,000 unique vectors for nearly 500,000 words which was derived from pre-trained word vectors from the <a href="https://nlp.stanford.edu/projects/glove/">GloVe Common Crawl</a> algorithm. Through this process, a vector of 300 features was generated for each individual tweet.</p>
<p>When it came to fine-tuning our model in order to out-perform the baseline model, we performed an optimization of hyperparameters for the feature extraction by using a grid search with the default 5-fold cross validation and scored using the F1-score metric.</p>
<p><strong>Results</strong>: Figure 14 provides an overview of the quantitative performance of our baseline models. The Linear SVC classifier reached the highest F1-score with 92.8% on the test set.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-28"></span>
<img src="figures/Table_4.png" alt="Overview of F1-scores for ML classifiers without parameter tuning" width="100%" />
<p class="caption">
Figure 14: Overview of F1-scores for ML classifiers without parameter tuning
</p>
</div>
</div>
<p>By fine-tuning our models, we beat our baseline model. The SVM classifier outperformed the other models with a F1-score of 93.5%</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-29"></span>
<img src="figures/Table_5.png" alt="Overview of F1-scores on test data for fine-tuned ML classifiers without word embedding" width="100%" />
<p class="caption">
Figure 15: Overview of F1-scores on test data for fine-tuned ML classifiers without word embedding
</p>
</div>
</div>
<p>And when we include word embedding the SVM classifier achieved an F1-score of 93.8% Even though including word embeddings improved our model, it drastically increased the runtime as it naturally enlargened the dataset.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:fig11"></span>
<img src="figures/Table_6.png" alt="Overview of F1-scores on test data for fine-tuned ML classifiers with word embedding" width="100%" />
<p class="caption">
Figure 16: Overview of F1-scores on test data for fine-tuned ML classifiers with word embedding
</p>
</div>
</div>
<p>Table 15 and 16 also display the results for testing the models on our new dataset. The SVM scores a result of 85.9% on the new data. The Logistic Regression classifier performs slightly better than SVM, obtaining an F1-score of 85.5% when applied to the new dataset. Overall, the F1-score dropped by more or less 10% for all of the models when applied to the new data.</p>
<h2 id="analysis">Analysis</h2>
<p>For further analysis we created scattertext plot visualizing which words and phrases in the training set correspond more often with real or fake tweets.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> The <span class="math inline">\(x\)</span>- and <span class="math inline">\(y\)</span>-axes are dense ranks for the terms wtih the highest level of association with real and fake tweets respectively. One element that can be observed in the plot is that some terms which often correspond with real news (blue) are associated with India, while terms relating to American presidents are more often associated with fake news (red). It can also be observed that certain terms with a high association are hashtags or even Twitter handles (e.g. indafightscorona), which might suggest that models base their classification on them.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-30"></span>
<img src="figures/scatterplot_training_data.png" alt="example for dangerous fake news" width="100%" />
<p class="caption">
Figure 17: example for dangerous fake news
</p>
</div>
</div>
<p>Figures 18 and 19 present a random sample of the social media posts which were misclassified by our algorithm when applied to the test set. By manually analyzing them we could not detect a clear pattern that characterizes the false negatives and positives.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-31"></span>
<img src="figures/misclassified-FP.png" alt="False Positives" width="100%" />
<p class="caption">
Figure 18: False Positives
</p>
</div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-32"></span>
<img src="figures/misclassified-FN.png" alt="False Negatives" width="100%" />
<p class="caption">
Figure 19: False Negatives
</p>
</div>
</div>
<p>However, it is remarkable that fake social media posts which were misclassified as true are not easily classifiable as fake news but would require expert knowledge or further research. One reason for this is the occurrence of words like “cases” or “details” that are associated with real news and also appeared more often in the real posts from the training data.</p>
<p><strong>Ethics</strong>: Even the best model will always contain some rate, however small, of misclassified statements. It is therefore essential that even if the application of our model could achieve an overall improvement of readers’ insights in fake news, this aspect is kept in mind before deploying the model in a real-world context.</p>
<h2 id="conclusions">Conclusions</h2>
<p>After achieving a relatively high performance of 93.8% with SVM, the predictive power was reduced quite strongly when we applied it to our new fake and real news dataset (84.9%). We assume that this reduction, however, is not exclusively attributable to the concept drift (for example, there might be words in the new dataset that were not present in the training set). However, based on previous research on this, we suspect that concept drift strongly contributes to the prediction loss on the new data.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>We thank <em>Patwa et al.</em> for providing the CONSTRAINT-2021 dataset which we used to train and test our models as well as used as a benchmark for our new data <span class="citation" data-cites="patwa2021fighting">(<a href="#ref-patwa2021fighting" role="doc-biblioref">Patwa et al. 2021</a>)</span>.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-horne2020" class="csl-entry" role="doc-biblioentry">
Horne, Benjamin D., Jeppe Nørregaard, and Sibel Adali. 2019. <span>“Robust Fake News Detection over Time and Attack.”</span> <em>ACM Trans. Intell. Syst. Technol.</em> 11 (1). <a href="https://doi.org/10.1145/3363818">https://doi.org/10.1145/3363818</a>.
</div>
<div id="ref-patwa2021fighting" class="csl-entry" role="doc-biblioentry">
Patwa, Parth, Shivam Sharma, Srinivas Pykl, Vineeth Guptha, Gitanjali Kumari, Md Shad Akhtar, Asif Ekbal, Amitava Das, and Tanmoy Chakraborty. 2021. <span>“Fighting an Infodemic: Covid-19 Fake News Dataset.”</span> In <em>International Workshop onCombating Online Hostile Posts in Regional Languages During Emergency Situation</em>, 21–29. Springer.
</div>
<div id="ref-varma2021systematic" class="csl-entry" role="doc-biblioentry">
Varma, Rajshree, Yugandhara Verma, Priya Vijayvargiya, and Prathamesh P Churi. 2021. <span>“A Systematic Survey on Deep Learning and Machine Learning Approaches of Fake News Detection in the Pre-and Post-COVID-19 Pandemic.”</span> <em>International Journal of Intelligent Computing and Cybernetics</em>.
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>The competition can be accessed here: <a href="https://competitions.codalab.org/competitions/26655" class="uri">https://competitions.codalab.org/competitions/26655</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Package repository can be accessed here: <a href="https://github.com/JasonKessler/scattertext" class="uri">https://github.com/JasonKessler/scattertext</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
<h3 id="references">References</h3>
<div id="references-listing"></div>
<h3 id="updates-and-corrections">Corrections</h3>
<p>If you see mistakes or want to suggest changes, please <a href="https://github.com/smkerr/COVID-fake-news-detection/issues/new">create an issue</a> on the source repository.</p>
<h3 id="reuse">Reuse</h3>
<p>Text and figures are licensed under Creative Commons Attribution <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>. Source code is available at <a href="https://github.com/smkerr/COVID-fake-news-detection">https://github.com/smkerr/COVID-fake-news-detection</a>, unless otherwise noted. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: "Figure from ...".</p>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
