---
title: "COVID Fake News Detection"
description: |
  Add a short description of your article here (i.e. "We present a deep learning approach to classifying labeled texts and phrases in party manifestos, using the coding scheme and documents from the Manifesto Project Corpus.") 
author: 
  - name: Marco Schmidt 
    url: https://allisonkoh.github.io
  - name: Hannah Schweren
  - name: Steve Kerr
    url: https://smkerr.github.io/
date: "`r Sys.Date()`" 
categories: 
  - Natural Language Processing 
creative_commons: CC BY # APPROPRIATE LICENSE?
repository_url: https://github.com/smkerr/COVID-fake-news-detection
output: 
  distill::distill_article: 
    self_contained: false
preview: figures/BERTfig3.png # UPDATE
bibliography: bibliography.bib # UPDATE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Load dependencies 
library(reticulate) # For rendering Python code 
```

## Abstract 

Fake news represents one of the most pressing issues faced by social media ecosystems today. While there have been many examples of fake news consumption leading to the adoption of inaccurate beliefs and harmful behaviors, fake news regarding the Covid-19 pandemic is arguably more dangerous since it may lead to worse public health outcomes.

While machine learning has advanced our ability to identify and root out fake news, this approach is not with- out its limitations. Fake news, by its very nature, is con- stantly evolving. A machine learning algorithm capable of detecting fake news with high effectiveness at one point in time may become significantly less effective when applied to news sampled from a later time period, a phenomenon referred to as “concept drift.”

Using the Covid-19 Fake News dataset which comprises 10,700 hand-labeled Covid-related social media posts (fake vs. real), we ask the following question: To what extent does the context-dependent and fast-moving nature of fake news represent a limitation for ML models?

For the first section, we create a fake news detection algorithm using existing Covid-related real and fake news datasets. The best performing model was Linear SVC, reaching a F1-score of 0.9345. For the second section, we measure the decay of our fake news detection algorithm by applying it to our own dataset comprised of recent Covid-related posts. On the new dataset the model reached a F1-score of 0.8139, indicating the presence of a substantial amount of concept drift when it comes to Covid-related social media posts. 

## Introduction / Background

Misinformation has always been a threat to society and democracy.^[Our GitHub repository can be accessed here: https://github.com/smkerr/COVID-fake-news-detection] Today, this danger is fueled by the possibility of its rapid diffusion through social media. Our project will therefore focus on the identification of such so-called “fake news” on social media platforms. Machine learning (ML) and Natural Language Processing (NLP) enable us to detect fake news so that we might warn the reader and raise awareness about misinformation [@lazer2018science].

While some ML models are already very efficient at classifying whether news is real or fake, such models still face limitations. ML models are often bound to the data upon which they were trained, which is why the fast-moving nature of online content could degrade their effectiveness. Seeing as fake news is highly influenced by current events, we hypothesize that temporally rigid training sets are prone to a decline in effectiveness. For our project we are focusing on the topic of Covid-19, because it is a current, politicized, and highly relevant topic and therefore creates room for misinformation and intentional manipulation in social media.

Our project comprises two main components. The first stage of our project will involve using with the Covid-19 Fake News Detection dataset to create an end-to-end ML project with a model for fake news detection which has a high-level of effectiveness. This part is based on similar work performed by *Dipta Das et al.* who used the same dataset to create fake news detection models [@dipta2021heuristic].

In the second stage, we address the aforementioned is- sue of the time- and context-related decline in effectiveness. Specifically, we attempt to answer the following research question: *To what extent does the contextual and fast-moving nature of fake news represent a limitation for ML models, specifically in the context of Covid-19 related social media data?* For this, we replicate the methods used by the authors of the original dataset to create our own more recent dataset.

We use methods consistent with those employed by the authors of the original dataset to label the new data (“real” or “fake”). We then apply the model we developed in the first section to both the original dataset and the new dataset and compare performance metrics to approximate the amount by which “concept drift” (and potentially other influences in the data sampling method) has degraded our model. By examining the way in which fake news detection algorithms decay over time, we hope to better understand the limitations of applying ML to the issue of fake news.

## Proposed Method 

This section details our approach to determining the extent to which concept drift degrades the quality of fake news detection algorithms. To measure concept decay, we require datasets from two distinct time periods. As discussed in further detail in Section 4: Experiments, we use the Covid-19 Fake News Detection dataset as our original dataset. We use methods consistent with those of the authors of the original dataset to create our own dataset which comprises more recent Covid-related social media posts. We replicate the methods of the original authors as closely as possible, so as to hold everything constant regarding the datasets aside from the timeframes in which they were collected. 

We use the original dataset to train and tune a binary classification algorithm capable of correctly detecting fake news at a competitive rate. As part of training the algorithm, we pre-process the text data. Pre-processing involves standard tasks such as tokenization, lemmatization, as well as removing non-alphanumeric characters and stopwords in order to make the contents of each tweet more usable. Furthermore, certain tweets require the  removal of URLs and HTML escape code. The motivation here to is to reduce the contents of each tweet to an amount which maximizes the amount of information that can be obtained using a ML algorithm.

Once the text data has been pre-processed, we count vectorize dataset, thereby creating a matrix of token counts for each tweet. We count vectorize both unigrams (i.e., individual words) and bigrams (i.e., pairs of neighboring words) in order to take the words themselves but also their contexts into consideration. We then transform the count matrix into a normalized term frequency-inverse document fequency (TF-IDF) representation.

The TF-IDF formula consists of two parts. The term frequency (TF) represents the number of instances that a word appears in a tweet. The inverse document frequency (IDF) is computed by taking the logarithm of the total number of tweets divided by the number of tweets which contain a given word. Where t denotes the terms, d denotes each tweet, df(t) denotes the frequency of a given word in the tweet, and n represents the total number of tweets. One is added to the numerator and denominator to prevent zero divisions. As a result, the TF-IDF for a word increases as its frequency in a tweet increases and decreases as the number of tweets in the dataset containing that word increases.

To extract further information from our text data, we transform the words from each tweet into word vectors or "word embeddings" in order to create meaning representations for each word. We then take the average of all the word vectors for each tweet to create a meaning representation for the overall tweet. Word embeddings are obtained using Global Vector for Word Representation (GloVe) pre-trained word vectors developed by researchers at Stanford University using an unsupervised learning algorithm in order to derive the relationship between words. These word embeddings combined with the TF-IDF features represent our feature set. 

To obtain our baseline, we create several common classification algorithms including Linear SVC, Logistic Regression, Gradient Boosting, and a Decision Tree classifier using default settings. This is consistent with the methods employed by *Patwa et al.* in their original paper. [@patwa2021fighting] As discussed in further detail in Section 4: Experiments, the Linear SVC with *sklearn*'s default parameters represents the most competitive model, obtaining a F1-score of 0.9281 when applied to the test set. As such, 0.9281 represents our baseline score which we aimed to exceed.

To create a fake news detection algorithm capable of outperforming the baseline model, we trained several classification algorithms which are often applied to fake news detection problems including SVM, Logistic Regression, XGBoost, AdaBoost, and a Voting Classifier which combines the aforementioned classification algorithms. These classifiers were trained on the original dataset's training set. We then use the validation set to evaluate performance, optimize hyperparameters, and to troubleshoot issues of model underfitting/overfitting. Once we had calibrated our best model, we applied to the test set to evaluate its performance. This concludes the first objective of our project: creating a competitive fake news detection algorithm.

In order to answer our key academic question regarding the extent to which the context-dependent and fast-moving nature of fake news represents a limitation for ML models, the second objective of our project, we start by using the F1 metric obtained from applying our best model to the original dataset's test set to establish our algorithm's baseline performance. We then apply the same algorithm to our new data. 

If our assumptions hold, then the difference between the F1-scores obtained when the algorithm is applied to the original dataset versus the new dataset represents an estimate of the extent to which concept drift has degraded the quality of our fake news detection algorithm. In doing so, we gain insight into the nature of fake news and the limitations associated with ML models' attempt to counter it.

```{r workflow, eval = TRUE, echo = FALSE, out.width = '100%', fig.cap = "Workflow"}
knitr::include_graphics("figures-ours/Diagramm_v2.png")
```

## Related Work 

The literature offers many similar approaches, for example, *Varma, Rajshree et al.* conducted a review of various ML and Deep Learning methods applied to the same topic coming to the conclusion, that "Naive Bayes, Support Vector Machine, and Logistic regression are the most widely used supervised ML algorithms for developing fake news detection models" [@varma2021systematic]. They also address the issue that the quality of the prediction naturally depends on the quality of the training data. So, the problem of limited amount of labeled data can pose a problem here. Further, they state that traditional models are outperformed by deep learning models. this means that our classifier cannot keep up with state of the art models but still provides relevant results.

As mentioned in the introduction, the idea for the first part of our project is based on the paper of *Patwa et al.* [@patwa2021fighting] which provided a good starting point for our further experimenting with the original and the new dataset. They get their best result with a Support Vector Machine, reaching 0.9281.

We hypothesized that there would be a reduction in the predictive power of our model over time. This assumption was based on previous research on the phenomenon of "concept drift" [@horne2020]. We assume that fake news constitutes non-stationary data whose characteristics "might shift over time because the spreaders of false news are conscious of the fact that automatic detection systems could detect them" [@choras2021]. This is related to the problem that *Raza et al.* diagnose as being a rather common challenge, stating that "most models are trained on the current data (...) which may not generalize to future events" [@raza2022fake].

A number of researchers analysed the issue of concept drift on fast-evolving topics such as Covid-19 and highlight that stationary data might not be appropriate to adapt to the continuous changes. *Zhang et al.* took a similar approach to ours to work on the topic of fake news, stating that a gap of two years leads to a significant concept drift [@9073558]. They also evoke the ethical dimension of this as the temporal component of fake news classification weakens the effectiveness of models over time and thus provide less reliable results. *Isaksson* dedicates their research, as we do, specifically to the subject of Covid-19 and the impact of concept drift in this area. They provide an overview of more elaborate techniques to detect concept drift [@isaksson2020].

## Experiments 

**Original Data**: We use the Covid-19 Fake News Detection dataset created in 2020 by *Patwa et al.* and featured in the Constraint\@AAAI2021 competition [@patwa2021fighting].^[The competition can be accessed here: https://competitions.codalab.org/competitions/26655] The dataset comprises 10,700 social media posts from various platforms including Twitter, Facebook, and Instagram. All content is related to the topic of Covid-19 and only English content is considered. Each post contains a falsifiable statement which has been labeled either "real'' or "fake.'' 

*Patwa et al.*'s Covid-19 Fake News Detection dataset has already been split into training (60%), test (20%), and validation sets (20%). The dataset is approximately balanced between real (52.3%) and fake (47.7%). Details regarding the distribution of real and fake posts across training, test, and validation sets can be found in Table 1.

"Fake'' claims are collected from various fact-checking websites such as PolitiFact, Snopes, Boomlive, etc. and from tools like Google fact-check-explorer and IFCN chatbot. "Real'' claims include tweets from credible and relevant Twitter accounts such as the World Health Organization (WHO) and the Centers for Disease Control and Prevention (CDC) among others. 

We acknowledge the shortcomings associated with sourcing "real'' data exclusively from Twitter while sourcing "fake'' data from various social media platforms. To correct for this, we remove all social media posts exceeding Twitter's 280 character limit from the dataset during pre-processing in an effort to make the "real" and "fake" data more comparable. While this did reduce the total number of observations, the amount of data is still sufficient to proceed. Moreover, we maintain the roughly equal split between real (49.1%) and fake (50.9%) social media posts in the dataset. See Table 2 for a more detailed breakdown.

To familiarize ourselves with the data and to detect potential biases, we conducted an exploratory analysis. Our analysis revealed that the number of characters (Figure 3) and the number of words (Figure 4) are approximately balanced between real and fake posts.

Additionally, we examined high-frequency words (Figure 2) for both fake and real claims to gain further insight into the data. No significant discrepancies could be identified. 

Lastly, we used part-of-speech tagging (Table 2) to gain a deeper understanding of the tweets. Besides a slight imbalance in the frequency of nouns between the real (51.3%) and fake (59.7%) posts, the overall balance gave no indication of significant biases in the dataset (Table 3).

In sum, our exploratory analysis did not uncover any significant underlying issues with the Covid-19 Fake News Detection dataset, thereby allowing us to proceed with our project. 

**New data**: For the second part of the project, we created our own dataset of more recent fake and real posts which were published between January and April 2022. We employed methods which closely mirror those used in the original paper by *Patwa et al.* so as to ensure that our dataset is comparable to the original dataset in all aspects aside from time period. Thus, we collected tweets from the same Twitter accounts and used Politifact.com as a source for fake news, as the authors did for the original dataset. 

While the authors of the original dataset did use additional fact-checking sources beyond PolitiFact when compiling fake claims such as Snopes, Boomlive, and Google fact-check-explorer among others, due to time constraints, we only use PolitiFact.com. Despite this, we were still able to create a large enough dataset to draw conclusions. In the creation of our dataset, we took special care to maintain an equal share of fake and real posts within the dataset (the final new dataset contains 100 real and 100 fake posts). An overview of the data collection process is provided in Figure 5.

**Evaluation method**: The fake news detection algorithm that we build during the the first part of our project is evaluated using a F1-score. We consider our fine-tuned model to be successful if it achieves a higher F1-score than our baseline model which received a F1-score of 0.9281. The F1-score is one of the most important measures for machine learning model's effectiveness as it provides the harmonic mean of the precision and recall. Thus it is a good indicator for our models' performance.

In this formula, $TP$ represents True Positives (i.e., real posts correctly classified as real), whereas  $FP$ represents False Positives (i.e., fake posts which were wrongly classified as real) and the $FN$ represents False Negatives (i.e., real posts which were wrongly classified as fake).

The F1-score is also present in the second part of our project where we attempt to quantify the amount of "concept drift" associated with Covid-related fake news. As explained in Section 3: Proposed Methods, we apply our fine-tuned fake news detection algorithm which was developed during the first part of the project to both the original dataset and the new dataset which we have created ourselves. The difference between the two F1-scores indicates the magnitude of the concept drift associated with Covid-related fake news between the two time periods in which the datasets were created.

While we acknowledge that there are several reasons which could explain the difference in F1-scores such as inconsistencies in the way both datasets were created, we believe that any gap between the two F1-scores contains valuable information regarding the magnitude to which concept drift may be present. 

**Software**: The project was conducted using the Python programming language. Besides fundamental Python packages such as pandas and *numpy*, we use *scikit-learn* for pre-processing text data, performing grid searches, training ML classifiers, and displaying results. Additionally, our *XGBoost* models were based on the  open-source software library of the same name. *NLTK* is used for NLP-related tasks such as pre-processing text data. *SpaCy* is used for additional NLP-related techniques which, in turn, relied upon the GloVe pre-trained word vectors for extracting word embeddings from the text data. Additionally, the *HyperOpt* package was used for hyper-parameter optimization. Furthermore, we used Google Colab for drafting our code, GitHub for version control and collaboration, as well as Overleaf for project documentation.

**Experimental details**: When creating our baseline model, we selected off-the-shelf models including Logistic Regression, Decision Tree, Gradient Boosting, and Linear SVC classifiers. We used default parameters for all models when identifying the most highest-scoring model (based on on F1-score) to serve as our baseline. 

For the word embedding feature extraction process, we relied upon *SpaCy*'s English-language reduced word vector table with 20,000 unique vectors for nearly 500,000 words which was derived from pre-trained word vectors from the GloVe Common Crawl. Through this process, a vector of 300 dimensions was generated based on the text inputs of each individual tweet. These were transformed into feature columns and used to improve our model beyond what would be possible using only TF-IDF features.

When it came to fine-tuning our own model to out-perform the baseline model, we performed an optimization of hyperparameters for the feature extraction by using a grid search with the default 5-fold cross validation and scored based on the F1-score metric. 

The optimization of the Support Vector Classifier (SVC), Logistic Regression, and AdaBoost classifiers was performed using cross-validation and grid search. Based on the results, the highest performing parameters for a SVM classifier was a n-gram range of (1, 2) with the regularization parameter set equal to 10 while using the values of $y$ to automatically adjust weights inversely proportional to class frequencies in the input data as follows:
$$\frac{n\_samples}{(n\_classes * np.bincount(y))}$$ 
The SVC kernel was also set equal to "linear" and the probability estimates enabled so that such estimates might be incorporated into the soft Voting Classifier at a subsequent stage. The computationally expensive probability estimates were partly compensated by increasing the the size of the kernel cache.

For the Logistic Regression classifier, our grid search showed that limiting the number of iterations to 1000, using SAGA (i.e., a modified form of the Stochastic Average Gradient) as the solver, and removing the regularization parameter optimized our model.

For the AdaBoost classifier, our grid search revealed that setting the number of estimators equal to 500 and the learning rate equal to 1 significantly improved model performance. The AdaBoost model also benefited from slightly modifying the pre-processing pipeline so that it would receive bigrams (i.e., word pairings) rather than unigrams (i.e., individual words) as input. 

Lastly, the optimization of the XGBoost classifier was performed with an HyperOpt algorithm which involved minimizing the AUC score. This hyperparameter optimization revealed that setting maximum depth of a tree equal to 3, the minimum hessian needed for a child to be created equal to 3, the number of estimators equal to 550, the learning rate equal to 0.2, and randomly sampling 85% of the training set prior to growing trees would optimize model performance. 

The Voting Classifier combines all other fine-tuned classifiers: SVM, Logistic Regression, AdaBoost and XGBoost. The voting decisions are based on class probabilities of the different classifiers (i.e., soft voting).

**Results**: To create our baseline, we fit Logistic Regression, Decision Tree, Gradient Boosting and Linear SVC classifiers to our training set. Table 4 provides an overview of the quantitative performance of our baseline models. The Linear SVC classifier reached the highest F1-score with 0.9281 on the test set.

By fine-tuning our models with the aforementioned parameters we beat our baseline model and achieved the results presented in table 5 and 6. The SVM classifier outperformed the other models with a F1-score of 0.9345 (without word embedding ) and 0.9383 (with word embedding). Even though including word embeddings improved our model, it also drastically increased the runtime as it naturally enlargened the dataset.

Table 4 and 5 further show the results for testing the models on our new dataset. The SVM scores a result of 0.8494 on the new data. The Logistic Regression classifier performs slightly better than SVM, obtaining an F1-score of 0.8546 when applied to the new dataset. Overall, the F1-score dropped by more or less 0.1 for all of the models when applied to the new data.

**Comment on quantitative results**: The results match our expectations. We can see that, mirroring the results of *Patwa et al.*. Even though all models provide acceptable results, the SVM model possesses the highest F1-score. [@patwa2021overview]. Moreover, the discovery that the use of bigrams instead of unigrams further improved the performance of SVM model matches our expectation since word proximity can be expected to contain important information regarding the context of words based on their proximity to one another. 

The addition of the word embeddings improved the F1-score across the board for all classifiers and an all datasets. Thus, word embedding have once again proven their capability of boosting the performance of ML classifiers when it comes to NLP tasks.

With regard to the tuning of hyper-parameters, an improvement was also expected, however the observed improvement compared to the baseline is not as substantial as previously anticipated. On the contrary, the performance of the Voting Classifier is worse than expected since it doesn't exceed the SVM classifier. Overall, we have experienced constraints in tuning classifiers by conventional means. Solutions to achieve further improvements, like word embedding, required more extensive pre-processing, which is, by its very nature, more computationally expensive.

## Analysis 

For further analysis we created scattertext plot^[Package Repository can be accessed here: https://github.com/JasonKessler/scattertext]
(Figure 6) visualizing which words and phrases in the training set correspond more often with real or fake tweets. The $x$- and $y$-axes are dense ranks for the mostly associated terms with regard to their association with real and fake tweets respectively. One element that can be observed in the plot is that some terms which often correspond with real news (blue) are associated with India, while terms relating to American presidents are more often associated with fake news. 

It can also be observed that certain terms with a high association are hashtags or even Twitter handles (e.g. indafightscorona), which might suggest that models base their classification on them. This raises the question of whether hashtags and Twitter handles should be removed in the pre-processing stage. There appears to be no simple answer, since hashtag and Twitter handles can also replace words in tweets, in which case excluding the contents of these hashtags would involve discarding potentially very useful information.

A scattertext plot was also created for the new dataset. However, given the very limited amount of tweets, the density ranks show less association for terms and is therefore less informative.

A preliminary error analysis conducted on the validation set allowed us to detect some duplicates in the dataset. We decided to remove those as they could cause a distortion of our model training and results.

Figures 7 and 8 present a random sample of the social media posts which were misclassified by our algorithm when applied to the test set. By manually analyzing them we could not detect a clear pattern that characterizes the false negatives and positives. 

However, it is remarkable that fake social media posts which were misclassified as true are not easily classifiable as fake news but would require expert knowledge or further research. One reason for this is the occurrence of words like "cases" or "details" that are associated with real news and also appeared more often in the real posts from the training data. (Figure 9 in the Appendix provide an overview of the quantity of words in fake and real news for the new dataset)

**Ethics**: When conducting a project on fake news, it is important to also consider the ethical background. First, we would like to discuss the data collection process for our second part, as web-scraping always contains an ethical dimension. We followed all the rules of polite scraping, by using APIs when possible (in the case of Twitter), and by only scraping small amounts of data.

Regarding the context of fake news classification in general, further concern about ethical issues would be appropriate for a larger-scale work that goes beyond our project. For example, while we have an appropriate proportion of false negatives and false positives for a machine learning algorithm, in real-world even the smallest rate of misclassification could pose a problem for the readers and instead of helping could lead them to be further misled. This is exactly the opposite of our desired outcome, and even the best model will always contain a rate, however small, of misclassified statements. It is therefore essential that even if the application of our model could achieve an overall improvement of readers insights in fake news, that this aspect is kept in mind before deploying the model in a real-world context.


## Conclusion(s)

We can summarize by confirming our hypothesis. After achieving a relatively high performance of 0.9383 with SVM, the predictive power was reduced quite strongly when we applied it to our new fake and real news dataset (0.8494). We assume that this reduction, however, is not solely attributable to the concept drift (for example, there might be words in the new dataset that were not present in the training set). However, based on previous research on this, we suspect that concept drift strongly contributes to the prediction loss on the new data.

In the course of our project, we were able to continuously increase our skill set. This of course refers to the set up of our end-to-end machine learning project, but it also includes working with the Twitter API, web-scraping from PolitiFact's website, manipulating Python data structures as well as other aspects of the Python programming language. It was encouraging that we were able to improve our model performance over the course of the project (albeit minimally) to beat our baseline model. 

The result of our project and the testing of our hypothesis could ideally be analyzed more thoroughly by conducting more detailed research that compares close to continuous time periods instead of just two different ones. This would allow a more meaningful analysis of the concept drift over a span of time to observe the expected gradual decreasing performance. Unfortunately, there was not enough time for this within the framework of this project. However, this represents an exciting area of potential future research.

## Acknowledgements

We thank *dipta2021heuristic* for providing the CONSTRAINT-2021 dataset which we used to train and test our models as well as used as a benchmark for our new data. 

################################################################################
## RMarkdown Tips

**Footnotes and Sidenotes**

You can use footnotes ^[This is a footnote. You can view this by hovering over the footnote in text.] or sidenotes to elaborate on a concept throughout the paper. 

<aside>
This is a side note. 
</aside>

**Below is an example of a figure:**

```{r fig1, eval = TRUE, echo = FALSE, out.width = '100%', fig.cap = "Model architecture"}
knitr::include_graphics("figures/BERTfig3.png")
```

***Note***: **Feel free to use some of the code from your project to explain your experiments. See example code block below.**

```{python bertcnn model parameters, echo = TRUE, eval = FALSE}
OUTPUT_DIM = len(LABEL.vocab)
DROPOUT = 0.5
N_FILTERS = 100
FILTER_SIZES = [2,3]

model = BERTCNN(bert,
                OUTPUT_DIM,
                DROPOUT,
                N_FILTERS,
                FILTER_SIZES)
```
